{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ccc02d20",
   "metadata": {},
   "source": [
    "# Enrollment Dataset – Data Concatenation and Cleaning\n",
    "\n",
    "## Overview\n",
    "This notebook focuses on preparing the **Aadhaar Enrollment Dataset** for downstream analysis as part of the UIDAI Data Hackathon.\n",
    "\n",
    "The enrollment data has been provided as **three separate CSV files**, which represent partitions of the same logical dataset. Before any meaningful analysis can be performed, these files must be **combined, validated, and cleaned** in a consistent and reproducible manner.\n",
    "\n",
    "## Objectives\n",
    "The objectives of this notebook are:\n",
    "\n",
    "1. To **load and inspect** all enrollment data files provided by UIDAI\n",
    "2. To **validate schema consistency** across the files\n",
    "3. To **concatenate** the files into a single unified dataset\n",
    "4. To perform **basic but essential data cleaning**, including:\n",
    "   - Standardizing column names\n",
    "   - Handling missing or malformed values\n",
    "   - Parsing date fields\n",
    "   - Removing structurally invalid records\n",
    "5. To generate a **clean, analysis-ready enrollment dataset** that will serve as a single source of truth for subsequent analysis\n",
    "\n",
    "## Scope and Design Principles\n",
    "- This notebook is **limited strictly to data preparation**\n",
    "- No exploratory analysis, modeling, or insights are derived here\n",
    "- All transformations are **transparent, minimal, and reversible**\n",
    "- No assumptions are made beyond what is necessary for data consistency\n",
    "\n",
    "## Output\n",
    "The final output of this notebook is a cleaned CSV file:\n",
    "\n",
    "03_Procesed_Data/enrollment_clean.csv\n",
    "\n",
    "\n",
    "This file will be used as input for all further exploratory and analytical notebooks.\n",
    "\n",
    "## Reproducibility\n",
    "All steps in this notebook are deterministic and can be rerun end-to-end using the raw input files, ensuring full reproducibility of results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fc2b15f",
   "metadata": {},
   "source": [
    "## Step 1: Environment Setup and Library Imports\n",
    "\n",
    "In this step, we import the required Python libraries and configure display settings to ensure consistent and readable outputs throughout the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "761508f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# Display configuration for better readability\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "pd.set_option(\"display.width\", 120)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "923c0189",
   "metadata": {},
   "source": [
    "## Step 2: Defining the Enrollment Data Source and File Inventory\n",
    "\n",
    "The Aadhaar Enrollment data has been provided as multiple CSV files, each representing a partition of the same dataset.\n",
    "Before loading the data, we explicitly define the data source location and enumerate all available enrollment files.\n",
    "\n",
    "This step ensures:\n",
    "- The correct directory structure is being used\n",
    "- All expected enrollment files are present\n",
    "- The data loading process is transparent and reproducible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6cb5652f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[WindowsPath('../01_Raw_Data_National/enrolment/api_data_aadhar_enrolment_0_500000.csv'),\n",
       " WindowsPath('../01_Raw_Data_National/enrolment/api_data_aadhar_enrolment_1000000_1006029.csv'),\n",
       " WindowsPath('../01_Raw_Data_National/enrolment/api_data_aadhar_enrolment_500000_1000000.csv')]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the path to the raw enrollment data\n",
    "ENROLLMENT_DATA_PATH = Path(\"../01_Raw_Data_National/enrolment\")\n",
    "\n",
    "# List all enrollment CSV files\n",
    "enrollment_files = sorted(ENROLLMENT_DATA_PATH.glob(\"*.csv\"))\n",
    "\n",
    "# Display the discovered files\n",
    "enrollment_files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e8a83e6",
   "metadata": {},
   "source": [
    "## Step 3: Loading Enrollment Data with Provenance Tracking\n",
    "\n",
    "In this step, we load each enrollment CSV file into memory and attach a provenance identifier to every record.\n",
    "Since the dataset has been split into multiple files for distribution purposes, it is important to retain information about the source file for traceability and validation.\n",
    "\n",
    "Adding provenance at this stage allows:\n",
    "- Verification of successful concatenation\n",
    "- Debugging of anomalies, if any, at the file level\n",
    "- Transparency in the data preparation process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a4ecbf96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize a list to store individual enrollment DataFrames\n",
    "enrollment_dfs = []\n",
    "\n",
    "# Load each enrollment file and add provenance information\n",
    "for file_path in enrollment_files:\n",
    "    df = pd.read_csv(file_path)\n",
    "    df[\"source_file\"] = file_path.name\n",
    "    enrollment_dfs.append(df)\n",
    "\n",
    "# Confirm the number of loaded DataFrames\n",
    "len(enrollment_dfs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "324bb6d7",
   "metadata": {},
   "source": [
    "## Step 4: Schema Validation Across Enrollment Files\n",
    "\n",
    "Before combining the enrollment data files, it is essential to verify that all files share a consistent schema.\n",
    "Schema validation ensures that each column represents the same attribute across all partitions and prevents silent data corruption during concatenation.\n",
    "\n",
    "In this step, we compare:\n",
    "- Column names\n",
    "- Column counts\n",
    "- Column ordering (for reference)\n",
    "\n",
    "Any discrepancies identified at this stage must be understood and resolved before proceeding further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6d6da7d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Schema for file 1:\n",
      "['date', 'state', 'district', 'pincode', 'age_0_5', 'age_5_17', 'age_18_greater', 'source_file']\n",
      "--------------------------------------------------------------------------------\n",
      "Schema for file 2:\n",
      "['date', 'state', 'district', 'pincode', 'age_0_5', 'age_5_17', 'age_18_greater', 'source_file']\n",
      "--------------------------------------------------------------------------------\n",
      "Schema for file 3:\n",
      "['date', 'state', 'district', 'pincode', 'age_0_5', 'age_5_17', 'age_18_greater', 'source_file']\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extract column schemas from each enrollment DataFrame\n",
    "schemas = [df.columns.tolist() for df in enrollment_dfs]\n",
    "\n",
    "# Display schema details for comparison\n",
    "for idx, schema in enumerate(schemas, start=1):\n",
    "    print(f\"Schema for file {idx}:\")\n",
    "    print(schema)\n",
    "    print(\"-\" * 80)\n",
    "\n",
    "# Check if all schemas are identical\n",
    "all_schemas_identical = all(schema == schemas[0] for schema in schemas)\n",
    "all_schemas_identical"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5ee897d",
   "metadata": {},
   "source": [
    "## Step 5: Concatenating the Enrollment Dataset\n",
    "\n",
    "After confirming that all enrollment files share an identical schema, we proceed to concatenate them into a single unified dataset.\n",
    "This step combines all records while preserving row-level integrity and prepares the data for cleaning and standardization.\n",
    "\n",
    "The resulting dataset represents the complete enrollment population covered by the provided UIDAI data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3d7fdb83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1006029, 8)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Concatenate all enrollment DataFrames into a single DataFrame\n",
    "enrollment_combined = pd.concat(enrollment_dfs, ignore_index=True)\n",
    "\n",
    "# Inspect the shape of the combined dataset\n",
    "enrollment_combined.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5878c111",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "source_file\n",
       "api_data_aadhar_enrolment_0_500000.csv           500000\n",
       "api_data_aadhar_enrolment_500000_1000000.csv     500000\n",
       "api_data_aadhar_enrolment_1000000_1006029.csv      6029\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enrollment_combined[\"source_file\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64531106",
   "metadata": {},
   "source": [
    "## Step 6: Initial Data Quality Assessment and Cleaning Strategy\n",
    "\n",
    "Before applying any cleaning operations, it is important to assess the overall quality of the combined enrollment dataset.\n",
    "This step focuses on understanding data completeness, data types, and potential structural issues that may affect downstream analysis.\n",
    "\n",
    "The objective is not to aggressively modify the data, but to:\n",
    "- Identify columns with high levels of missing values\n",
    "- Inspect data types and detect obvious inconsistencies\n",
    "- Define a minimal and justified cleaning strategy\n",
    "\n",
    "All cleaning decisions made after this step are guided by evidence observed here.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ea5335c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1006029 entries, 0 to 1006028\n",
      "Data columns (total 8 columns):\n",
      " #   Column          Non-Null Count    Dtype \n",
      "---  ------          --------------    ----- \n",
      " 0   date            1006029 non-null  object\n",
      " 1   state           1006029 non-null  object\n",
      " 2   district        1006029 non-null  object\n",
      " 3   pincode         1006029 non-null  int64 \n",
      " 4   age_0_5         1006029 non-null  int64 \n",
      " 5   age_5_17        1006029 non-null  int64 \n",
      " 6   age_18_greater  1006029 non-null  int64 \n",
      " 7   source_file     1006029 non-null  object\n",
      "dtypes: int64(4), object(4)\n",
      "memory usage: 61.4+ MB\n"
     ]
    }
   ],
   "source": [
    "# High-level overview of the combined enrollment dataset\n",
    "enrollment_combined.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d83ef1a",
   "metadata": {},
   "source": [
    "## Step 7: Applying Minimal and Justified Data Cleaning\n",
    "\n",
    "Following dataset concatenation and schema validation, this step applies a series of minimal and evidence-based data cleaning operations.\n",
    "All actions in this step are strictly limited to structural correctness, data integrity, and consistency, without introducing analytical assumptions or feature engineering.\n",
    "\n",
    "Each sub-step below documents a specific validation or cleaning action performed on the enrollment dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "231672f4",
   "metadata": {},
   "source": [
    "### Step 7A: Robust Date Parsing\n",
    "\n",
    "Inspection of the enrollment dataset revealed that the date column contains valid values expressed in more than one official format,\n",
    "which is common in Indian administrative data.\n",
    "\n",
    "To prevent data loss and ensure accurate temporal analysis, a robust date parsing strategy was applied that:\n",
    "- Explicitly handles all observed date formats\n",
    "- Respects the day-first convention used in Indian datasets\n",
    "- Preserves all records without arbitrary deletion\n",
    "\n",
    "No rows were removed during this process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0584c6b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a clean copy to avoid mutating the combined dataset\n",
    "enrollment_clean = enrollment_combined.copy()\n",
    "\n",
    "# Attempt 1: Parse dates in DD-MM-YYYY format\n",
    "parsed_dash = pd.to_datetime(\n",
    "    enrollment_clean[\"date\"],\n",
    "    format=\"%d-%m-%Y\",\n",
    "    errors=\"coerce\"\n",
    ")\n",
    "\n",
    "# Attempt 2: Parse dates in D/M/YYYY format (Indian day-first convention)\n",
    "parsed_slash = pd.to_datetime(\n",
    "    enrollment_clean[\"date\"],\n",
    "    dayfirst=True,\n",
    "    errors=\"coerce\"\n",
    ")\n",
    "\n",
    "# Combine both parsing strategies\n",
    "enrollment_clean[\"date\"] = parsed_dash.fillna(parsed_slash)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c5918130",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.int64(0)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check for any unparsed dates\n",
    "enrollment_clean[\"date\"].isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ba86d26",
   "metadata": {},
   "source": [
    "### Step 7B: Validation of Age-Based Numerical Fields\n",
    "\n",
    "The enrollment dataset contains age-segmented numerical fields representing different age groups.\n",
    "Before further analysis, these fields were validated to ensure basic numerical integrity.\n",
    "\n",
    "The following checks were performed:\n",
    "- Verification that all age-based values are non-negative\n",
    "- Confirmation that no structurally invalid numerical values exist\n",
    "\n",
    "No transformations or corrections were required, as all values were found to be valid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "92ac42bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "age_0_5           0\n",
       "age_5_17          0\n",
       "age_18_greater    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Validate age columns contain no negative values\n",
    "age_columns = [\"age_0_5\", \"age_5_17\", \"age_18_greater\"]\n",
    "(enrollment_clean[age_columns] < 0).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0143002",
   "metadata": {},
   "source": [
    "### Step 7C: Standardization of State Names\n",
    "\n",
    "State names in the enrollment dataset exhibited semantic duplication due to legacy spellings,\n",
    "administrative reorganization, and formatting inconsistencies.\n",
    "\n",
    "To enable reliable state-level aggregation, all State values were normalized to a canonical set of officially recognized names using an explicit and documented mapping strategy.\n",
    "\n",
    "This process:\n",
    "- Eliminated duplicate representations of the same State\n",
    "- Preserved administrative correctness\n",
    "- Removed clearly invalid tokens\n",
    "\n",
    "All normalization decisions were applied transparently and consistently.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "59af1ab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize State column\n",
    "for col in [\"state\"]:\n",
    "    enrollment_clean[col] = (\n",
    "        enrollment_clean[col]\n",
    "        .astype(str)\n",
    "        .str.strip()          # remove leading/trailing whitespace\n",
    "        .str.title()          # standardize casing (e.g., 'karnataka' -> 'Karnataka')\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0fbb69a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check number of unique states\n",
    "enrollment_clean[\"state\"].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5f3c9f48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Meghalaya', 'Karnataka', 'Uttar Pradesh', 'Bihar', 'Maharashtra',\n",
       "       'Haryana', 'Rajasthan', 'Punjab', 'Delhi', 'Madhya Pradesh',\n",
       "       'West Bengal', 'Assam', 'Uttarakhand', 'Gujarat', 'Andhra Pradesh',\n",
       "       'Tamil Nadu', 'Chhattisgarh', 'Jharkhand', 'Nagaland', 'Manipur',\n",
       "       'Telangana', 'Tripura', 'Mizoram', 'Jammu And Kashmir',\n",
       "       'Chandigarh', 'Sikkim', 'Odisha', 'Kerala',\n",
       "       'The Dadra And Nagar Haveli And Daman And Diu',\n",
       "       'Arunachal Pradesh', 'Himachal Pradesh', 'Goa',\n",
       "       'Dadra And Nagar Haveli And Daman And Diu', 'Ladakh',\n",
       "       'Andaman And Nicobar Islands', 'Orissa', 'Pondicherry',\n",
       "       'Puducherry', 'Lakshadweep', 'Andaman & Nicobar Islands',\n",
       "       'Dadra & Nagar Haveli', 'Dadra And Nagar Haveli', 'Daman And Diu',\n",
       "       'Jammu & Kashmir', 'West  Bengal', '100000', 'Daman & Diu',\n",
       "       'West Bangal', 'Westbengal'], dtype=object)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enrollment_clean[\"state\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "55a313f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Canonical state name mapping\n",
    "state_normalization_map = {\n",
    "    # West Bengal variants\n",
    "    \"West Bengal\": \"West Bengal\",\n",
    "    \"West  Bengal\": \"West Bengal\",\n",
    "    \"West Bangal\": \"West Bengal\",\n",
    "    \"Westbengal\": \"West Bengal\",\n",
    "\n",
    "    # Odisha / Orissa\n",
    "    \"Orissa\": \"Odisha\",\n",
    "\n",
    "    # Jammu & Kashmir\n",
    "    \"Jammu & Kashmir\": \"Jammu And Kashmir\",\n",
    "\n",
    "    # Andaman & Nicobar\n",
    "    \"Andaman & Nicobar Islands\": \"Andaman And Nicobar Islands\",\n",
    "\n",
    "    # UT merger\n",
    "    \"Dadra & Nagar Haveli\": \"Dadra And Nagar Haveli And Daman And Diu\",\n",
    "    \"Daman & Diu\": \"Dadra And Nagar Haveli And Daman And Diu\",\n",
    "    \"Daman And Diu\": \"Dadra And Nagar Haveli And Daman And Diu\",\n",
    "    \"Dadra And Nagar Haveli\": \"Dadra And Nagar Haveli And Daman And Diu\",\n",
    "    \"The Dadra And Nagar Haveli And Daman And Diu\":\n",
    "        \"Dadra And Nagar Haveli And Daman And Diu\",\n",
    "\n",
    "    # Puducherry\n",
    "    \"Pondicherry\": \"Puducherry\"\n",
    "}\n",
    "\n",
    "# Apply normalization\n",
    "enrollment_clean[\"state\"] = (\n",
    "    enrollment_clean[\"state\"]\n",
    "    .replace(state_normalization_map)\n",
    ")\n",
    "\n",
    "# Remove clearly invalid state values\n",
    "invalid_states = [\"100000\"]\n",
    "\n",
    "enrollment_clean = enrollment_clean[\n",
    "    ~enrollment_clean[\"state\"].isin(invalid_states)\n",
    "]\n",
    "\n",
    "enrollment_clean[\"state\"].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "75e84748",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Meghalaya', 'Karnataka', 'Uttar Pradesh', 'Bihar', 'Maharashtra',\n",
       "       'Haryana', 'Rajasthan', 'Punjab', 'Delhi', 'Madhya Pradesh',\n",
       "       'West Bengal', 'Assam', 'Uttarakhand', 'Gujarat', 'Andhra Pradesh',\n",
       "       'Tamil Nadu', 'Chhattisgarh', 'Jharkhand', 'Nagaland', 'Manipur',\n",
       "       'Telangana', 'Tripura', 'Mizoram', 'Jammu And Kashmir',\n",
       "       'Chandigarh', 'Sikkim', 'Odisha', 'Kerala',\n",
       "       'Dadra And Nagar Haveli And Daman And Diu', 'Arunachal Pradesh',\n",
       "       'Himachal Pradesh', 'Goa', 'Ladakh', 'Andaman And Nicobar Islands',\n",
       "       'Puducherry', 'Lakshadweep'], dtype=object)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enrollment_clean[\"state\"].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d5cb382",
   "metadata": {},
   "source": [
    "### Step 7D: Structural Standardization of District Names\n",
    "\n",
    "District names in the dataset reflect diverse administrative reporting practices, historical naming conventions, and recent district formations.\n",
    "Given the frequency of district reorganization in India, full semantic normalization was intentionally avoided.\n",
    "\n",
    "Instead, district names were standardized at a structural level by:\n",
    "- Removing extraneous whitespace and formatting artifacts\n",
    "- Normalizing text casing\n",
    "- Eliminating clearly invalid non-district tokens\n",
    "\n",
    "No semantic remapping or merging of district names was performed to preserve the original reporting granularity.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4d17d442",
   "metadata": {},
   "outputs": [],
   "source": [
    "enrollment_clean[\"district\"] = (\n",
    "    enrollment_clean[\"district\"]\n",
    "    .astype(str)\n",
    "    .str.strip()\n",
    "    .str.replace(r\"\\s+\", \" \", regex=True)\n",
    "    .str.replace(r\"\\*\", \"\", regex=True)\n",
    "    .str.replace(r\"[()]\", \"\", regex=True)\n",
    "    .str.replace(\"–\", \"-\", regex=False)\n",
    "    .str.replace(\"−\", \"-\", regex=False)\n",
    "    .str.replace(\"?\", \"\", regex=False)\n",
    "    .str.title()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "648eacd1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "956"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "invalid_districts = {\n",
    "    \"East\", \"West\", \"North\", \"South\", \"North East\"\n",
    "}\n",
    "\n",
    "enrollment_clean = enrollment_clean[\n",
    "    ~enrollment_clean[\"district\"].isin(invalid_districts)\n",
    "]\n",
    "\n",
    "enrollment_clean[\"district\"].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c906f865",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop temporary provenance column used during ingestion\n",
    "if \"source_file\" in enrollment_clean.columns:\n",
    "    enrollment_clean = enrollment_clean.drop(columns=[\"source_file\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "fe219fd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 1005293 entries, 0 to 1006028\n",
      "Data columns (total 7 columns):\n",
      " #   Column          Non-Null Count    Dtype         \n",
      "---  ------          --------------    -----         \n",
      " 0   date            1005293 non-null  datetime64[ns]\n",
      " 1   state           1005293 non-null  object        \n",
      " 2   district        1005293 non-null  object        \n",
      " 3   pincode         1005293 non-null  int64         \n",
      " 4   age_0_5         1005293 non-null  int64         \n",
      " 5   age_5_17        1005293 non-null  int64         \n",
      " 6   age_18_greater  1005293 non-null  int64         \n",
      "dtypes: datetime64[ns](1), int64(4), object(2)\n",
      "memory usage: 61.4+ MB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>state</th>\n",
       "      <th>district</th>\n",
       "      <th>pincode</th>\n",
       "      <th>age_0_5</th>\n",
       "      <th>age_5_17</th>\n",
       "      <th>age_18_greater</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2025-03-02</td>\n",
       "      <td>Meghalaya</td>\n",
       "      <td>East Khasi Hills</td>\n",
       "      <td>793121</td>\n",
       "      <td>11</td>\n",
       "      <td>61</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2025-03-09</td>\n",
       "      <td>Karnataka</td>\n",
       "      <td>Bengaluru Urban</td>\n",
       "      <td>560043</td>\n",
       "      <td>14</td>\n",
       "      <td>33</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2025-03-09</td>\n",
       "      <td>Uttar Pradesh</td>\n",
       "      <td>Kanpur Nagar</td>\n",
       "      <td>208001</td>\n",
       "      <td>29</td>\n",
       "      <td>82</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2025-03-09</td>\n",
       "      <td>Uttar Pradesh</td>\n",
       "      <td>Aligarh</td>\n",
       "      <td>202133</td>\n",
       "      <td>62</td>\n",
       "      <td>29</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2025-03-09</td>\n",
       "      <td>Karnataka</td>\n",
       "      <td>Bengaluru Urban</td>\n",
       "      <td>560016</td>\n",
       "      <td>14</td>\n",
       "      <td>16</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        date          state          district  pincode  age_0_5  age_5_17  age_18_greater\n",
       "0 2025-03-02      Meghalaya  East Khasi Hills   793121       11        61              37\n",
       "1 2025-03-09      Karnataka   Bengaluru Urban   560043       14        33              39\n",
       "2 2025-03-09  Uttar Pradesh      Kanpur Nagar   208001       29        82              12\n",
       "3 2025-03-09  Uttar Pradesh           Aligarh   202133       62        29              15\n",
       "4 2025-03-09      Karnataka   Bengaluru Urban   560016       14        16              21"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enrollment_clean.info()\n",
    "enrollment_clean.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de699461",
   "metadata": {},
   "source": [
    "### Step 7 Summary\n",
    "\n",
    "At the conclusion of Step 7, the enrollment dataset satisfies the following conditions:\n",
    "- Dates are consistently parsed and temporally reliable\n",
    "- Numerical age fields are validated for integrity\n",
    "- State names are canonical and aggregation-safe\n",
    "- District names are structurally clean while preserving administrative semantics\n",
    "\n",
    "The dataset is now fully prepared for downstream exploratory and analytical tasks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78e05f29",
   "metadata": {},
   "source": [
    "## Step 8: Persisting the Clean Enrollment Dataset\n",
    "\n",
    "After completing all validation and cleaning steps, the enrollment dataset is now finalized.\n",
    "In this step, the cleaned dataset is persisted to disk to serve as the single source of truth for all downstream analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "98fce2c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WindowsPath('../03_Processed_Data/enrollment_clean.csv')"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define output directory for processed data\n",
    "OUTPUT_DIR = Path(\"../03_Processed_Data\")\n",
    "OUTPUT_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# Persist the cleaned enrollment dataset\n",
    "output_path = OUTPUT_DIR / \"enrollment_clean.csv\"\n",
    "enrollment_clean.to_csv(output_path, index=False)\n",
    "\n",
    "output_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c4d0df9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>state</th>\n",
       "      <th>district</th>\n",
       "      <th>pincode</th>\n",
       "      <th>age_0_5</th>\n",
       "      <th>age_5_17</th>\n",
       "      <th>age_18_greater</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2025-03-02</td>\n",
       "      <td>Meghalaya</td>\n",
       "      <td>East Khasi Hills</td>\n",
       "      <td>793121</td>\n",
       "      <td>11</td>\n",
       "      <td>61</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2025-03-09</td>\n",
       "      <td>Karnataka</td>\n",
       "      <td>Bengaluru Urban</td>\n",
       "      <td>560043</td>\n",
       "      <td>14</td>\n",
       "      <td>33</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2025-03-09</td>\n",
       "      <td>Uttar Pradesh</td>\n",
       "      <td>Kanpur Nagar</td>\n",
       "      <td>208001</td>\n",
       "      <td>29</td>\n",
       "      <td>82</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2025-03-09</td>\n",
       "      <td>Uttar Pradesh</td>\n",
       "      <td>Aligarh</td>\n",
       "      <td>202133</td>\n",
       "      <td>62</td>\n",
       "      <td>29</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2025-03-09</td>\n",
       "      <td>Karnataka</td>\n",
       "      <td>Bengaluru Urban</td>\n",
       "      <td>560016</td>\n",
       "      <td>14</td>\n",
       "      <td>16</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         date          state          district  pincode  age_0_5  age_5_17  age_18_greater\n",
       "0  2025-03-02      Meghalaya  East Khasi Hills   793121       11        61              37\n",
       "1  2025-03-09      Karnataka   Bengaluru Urban   560043       14        33              39\n",
       "2  2025-03-09  Uttar Pradesh      Kanpur Nagar   208001       29        82              12\n",
       "3  2025-03-09  Uttar Pradesh           Aligarh   202133       62        29              15\n",
       "4  2025-03-09      Karnataka   Bengaluru Urban   560016       14        16              21"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Verify saved file can be read correctly\n",
    "pd.read_csv(output_path, nrows=5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
