{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e4bc691f",
   "metadata": {},
   "source": [
    "# Biometric Update Dataset – Data Concatenation and Cleaning\n",
    "\n",
    "## Overview\n",
    "This notebook focuses on preparing the Aadhaar **Biometric Update Dataset** for downstream analysis as part of the UIDAI Data Hackathon.\n",
    "\n",
    "The biometric update data has been provided as multiple CSV files, each representing a partition of the same logical dataset.\n",
    "Before analysis, these files must be combined, validated, and cleaned in a consistent and reproducible manner.\n",
    "\n",
    "## Objectives\n",
    "The objectives of this notebook are to:\n",
    "\n",
    "1. Load and inventory all biometric update data files\n",
    "2. Validate schema consistency across file partitions\n",
    "3. Concatenate the files into a unified dataset\n",
    "4. Perform minimal and justified data cleaning, including:\n",
    "   - Date parsing (if applicable)\n",
    "   - Validation of biometric update counts\n",
    "   - Standardization of geographic attributes\n",
    "5. Persist a clean biometric update dataset for analysis\n",
    "\n",
    "## Scope and Design Principles\n",
    "- This notebook is strictly limited to data preparation\n",
    "- No exploratory or inferential analysis is performed here\n",
    "- Cleaning actions are evidence-driven and fully documented\n",
    "- Administrative semantics are preserved unless explicitly justified\n",
    "\n",
    "## Output\n",
    "The final output of this notebook is:\n",
    "\n",
    "03_Processed_Data/biometric_update_clean.csv\n",
    "\n",
    "This dataset will be used in downstream analytical notebooks.\n",
    "\n",
    "## Reproducibility\n",
    "All steps in this notebook are deterministic and can be rerun end-to-end using the raw source files.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b31b3cac",
   "metadata": {},
   "source": [
    "## Step 1: Environment Setup and Library Imports\n",
    "\n",
    "This step initializes the Python environment and imports the required libraries to ensure consistent data handling and display.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "02c08962",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "pd.set_option(\"display.width\", 120)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e18ef31c",
   "metadata": {},
   "source": [
    "## Step 2: Defining the Biometric Update Data Source and File Inventory\n",
    "\n",
    "The biometric update dataset is provided as multiple CSV files.\n",
    "This step identifies all available files and verifies that the expected number of partitions is present before loading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "618ff6dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[WindowsPath('../01_Raw_Data_National/biometric_update/api_data_aadhar_biometric_0_500000.csv'),\n",
       " WindowsPath('../01_Raw_Data_National/biometric_update/api_data_aadhar_biometric_1000000_1500000.csv'),\n",
       " WindowsPath('../01_Raw_Data_National/biometric_update/api_data_aadhar_biometric_1500000_1861108.csv'),\n",
       " WindowsPath('../01_Raw_Data_National/biometric_update/api_data_aadhar_biometric_500000_1000000.csv')]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define path to raw biometric update data\n",
    "BIOMETRIC_DATA_PATH = Path(\"../01_Raw_Data_National/biometric_update\")\n",
    "\n",
    "# List all biometric update CSV files\n",
    "biometric_files = sorted(BIOMETRIC_DATA_PATH.glob(\"*.csv\"))\n",
    "\n",
    "biometric_files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21b203b7",
   "metadata": {},
   "source": [
    "## Step 3: Loading Biometric Update Data with Provenance Tracking\n",
    "\n",
    "In this step, each biometric update CSV file is loaded into memory.\n",
    "A temporary provenance column is added to track the source file for each record, enabling validation of successful ingestion and concatenation.\n",
    "\n",
    "This column is used only during data preparation and is removed before persisting the final dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "978225a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load biometric update files with provenance tracking\n",
    "biometric_dfs = []\n",
    "\n",
    "for file_path in biometric_files:\n",
    "    df = pd.read_csv(file_path)\n",
    "    df[\"source_file\"] = file_path.name\n",
    "    biometric_dfs.append(df)\n",
    "\n",
    "# Confirm all files are loaded\n",
    "len(biometric_dfs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de5bea0c",
   "metadata": {},
   "source": [
    "## Step 4: Schema Validation Across Biometric Update Files\n",
    "\n",
    "Before concatenating the biometric update files, it is necessary to confirm that all partitions share a consistent schema.\n",
    "Schema validation ensures that each column represents the same attribute across all files and prevents silent data corruption during concatenation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e32055fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Schema for file 1:\n",
      "['date', 'state', 'district', 'pincode', 'bio_age_5_17', 'bio_age_17_', 'source_file']\n",
      "--------------------------------------------------------------------------------\n",
      "Schema for file 2:\n",
      "['date', 'state', 'district', 'pincode', 'bio_age_5_17', 'bio_age_17_', 'source_file']\n",
      "--------------------------------------------------------------------------------\n",
      "Schema for file 3:\n",
      "['date', 'state', 'district', 'pincode', 'bio_age_5_17', 'bio_age_17_', 'source_file']\n",
      "--------------------------------------------------------------------------------\n",
      "Schema for file 4:\n",
      "['date', 'state', 'district', 'pincode', 'bio_age_5_17', 'bio_age_17_', 'source_file']\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extract column schemas from each biometric update DataFrame\n",
    "schemas = [df.columns.tolist() for df in biometric_dfs]\n",
    "\n",
    "# Display schemas for comparison\n",
    "for idx, schema in enumerate(schemas, start=1):\n",
    "    print(f\"Schema for file {idx}:\")\n",
    "    print(schema)\n",
    "    print(\"-\" * 80)\n",
    "\n",
    "# Check whether all schemas are identical\n",
    "all_schemas_identical = all(schema == schemas[0] for schema in schemas)\n",
    "all_schemas_identical"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5372b86d",
   "metadata": {},
   "source": [
    "## Step 5: Concatenating the Biometric Update Dataset\n",
    "\n",
    "After confirming schema consistency across all biometric update files, the individual partitions are concatenated into a single unified dataset.\n",
    "This step consolidates all biometric update records and prepares the data for validation and cleaning.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9eb29a80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1861108, 7)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Concatenate all biometric update DataFrames\n",
    "biometric_combined = pd.concat(biometric_dfs, ignore_index=True)\n",
    "\n",
    "# Inspect the shape of the combined dataset\n",
    "biometric_combined.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "69cdf452",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "source_file\n",
       "api_data_aadhar_biometric_0_500000.csv           500000\n",
       "api_data_aadhar_biometric_1000000_1500000.csv    500000\n",
       "api_data_aadhar_biometric_500000_1000000.csv     500000\n",
       "api_data_aadhar_biometric_1500000_1861108.csv    361108\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Verify that all source files contributed data\n",
    "biometric_combined[\"source_file\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c913161",
   "metadata": {},
   "source": [
    "## Step 6: Initial Data Quality Assessment and Cleaning Strategy (Biometric Update)\n",
    "\n",
    "Before applying any cleaning operations, an initial assessment of the biometric update dataset is performed.\n",
    "This step focuses on understanding data completeness, data types, and potential structural issues that may affect downstream analysis.\n",
    "\n",
    "The objective is to identify:\n",
    "- Columns with missing or inconsistent values\n",
    "- Data types that require conversion (e.g., dates)\n",
    "- Fields that may require validation (e.g., biometric update counts)\n",
    "- Geographic attributes that require standardization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "45bb3e6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1861108 entries, 0 to 1861107\n",
      "Data columns (total 7 columns):\n",
      " #   Column        Dtype \n",
      "---  ------        ----- \n",
      " 0   date          object\n",
      " 1   state         object\n",
      " 2   district      object\n",
      " 3   pincode       int64 \n",
      " 4   bio_age_5_17  int64 \n",
      " 5   bio_age_17_   int64 \n",
      " 6   source_file   object\n",
      "dtypes: int64(3), object(4)\n",
      "memory usage: 99.4+ MB\n"
     ]
    }
   ],
   "source": [
    "# High-level overview of the biometric update dataset\n",
    "biometric_combined.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "84dba83a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "date            0\n",
       "state           0\n",
       "district        0\n",
       "pincode         0\n",
       "bio_age_5_17    0\n",
       "bio_age_17_     0\n",
       "source_file     0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "biometric_combined.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db4e4273",
   "metadata": {},
   "source": [
    "## Step 7: Applying Minimal and Justified Data Cleaning (Biometric Update)\n",
    "\n",
    "Based on the initial data quality assessment, the biometric update dataset is complete and structurally consistent.\n",
    "Cleaning actions in this step are intentionally minimal and focus on correctness, consistency, and cross-dataset alignment.\n",
    "\n",
    "The following operations are performed:\n",
    "- Robust parsing of the date column\n",
    "- Validation of biometric age-based count fields\n",
    "- Canonical standardization of State names (aligned with Enrollment and Demographic datasets)\n",
    "- Structural standardization of District names\n",
    "- Removal of temporary ingestion-related columns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53345092",
   "metadata": {},
   "source": [
    "### Step 7A: Robust Date Parsing\n",
    "\n",
    "The biometric update dataset contains date values stored as text and expressed in more than one valid format.\n",
    "To ensure reliable temporal analysis and avoid data loss, a robust date parsing strategy is applied that:\n",
    "- Handles all observed date formats\n",
    "- Respects the day-first convention used in Indian administrative datasets\n",
    "- Preserves all valid records\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "29f844a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.int64(0)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Work on a clean copy\n",
    "biometric_clean = biometric_combined.copy()\n",
    "\n",
    "# Attempt 1: DD-MM-YYYY\n",
    "parsed_dash = pd.to_datetime(\n",
    "    biometric_clean[\"date\"],\n",
    "    format=\"%d-%m-%Y\",\n",
    "    errors=\"coerce\"\n",
    ")\n",
    "\n",
    "# Attempt 2: Day-first flexible parsing\n",
    "parsed_slash = pd.to_datetime(\n",
    "    biometric_clean[\"date\"],\n",
    "    dayfirst=True,\n",
    "    errors=\"coerce\"\n",
    ")\n",
    "\n",
    "# Combine both parsing attempts\n",
    "biometric_clean[\"date\"] = parsed_dash.fillna(parsed_slash)\n",
    "\n",
    "# Validate parsing success\n",
    "biometric_clean[\"date\"].isna().sum()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a0ef922",
   "metadata": {},
   "source": [
    "### Step 7B: Validation of Biometric Age Fields\n",
    "\n",
    "Biometric update records include age-segmented counts.\n",
    "These fields are validated to ensure numerical integrity prior to analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "e0ba18a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bio_age_5_17    0\n",
       "bio_age_17_     0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bio_age_cols = [\"bio_age_5_17\", \"bio_age_17_\"]\n",
    "\n",
    "# Check for negative values\n",
    "(biometric_clean[bio_age_cols] < 0).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30ce652b",
   "metadata": {},
   "source": [
    "### Step 7C: Standardization of State Names\n",
    "\n",
    "State names are normalized using the canonical mapping adopted across all datasets\n",
    "to ensure consistent state-level aggregation and comparison.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "c41b4a5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize State column\n",
    "for col in [\"state\"]:\n",
    "    biometric_clean[col] = (\n",
    "        biometric_clean[col]\n",
    "        .astype(str)\n",
    "        .str.strip()          # remove leading/trailing whitespace\n",
    "        .str.title()          # standardize casing (e.g., 'karnataka' -> 'Karnataka')\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "7a001370",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check number of unique states\n",
    "biometric_clean[\"state\"].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "87f7615c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Haryana', 'Bihar', 'Jammu And Kashmir', 'Tamil Nadu',\n",
       "       'Maharashtra', 'Gujarat', 'Odisha', 'West Bengal', 'Kerala',\n",
       "       'Rajasthan', 'Punjab', 'Himachal Pradesh', 'Uttar Pradesh',\n",
       "       'Assam', 'Uttarakhand', 'Madhya Pradesh', 'Karnataka',\n",
       "       'Andhra Pradesh', 'Telangana', 'Goa', 'Nagaland', 'Jharkhand',\n",
       "       'Delhi', 'Chhattisgarh', 'Meghalaya', 'Chandigarh', 'Orissa',\n",
       "       'Puducherry', 'Pondicherry', 'Manipur', 'Sikkim', 'Tripura',\n",
       "       'Mizoram', 'Arunachal Pradesh', 'Ladakh',\n",
       "       'Dadra And Nagar Haveli And Daman And Diu', 'Daman And Diu',\n",
       "       'Andaman And Nicobar Islands', 'Andaman & Nicobar Islands',\n",
       "       'Dadra And Nagar Haveli', 'Lakshadweep', 'Daman & Diu',\n",
       "       'Dadra & Nagar Haveli', 'Jammu & Kashmir', 'Westbengal',\n",
       "       'West  Bengal', 'West Bangal', 'Uttaranchal', 'Chhatisgarh',\n",
       "       'Tamilnadu'], dtype=object)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "biometric_clean[\"state\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "414563a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Canonical state name mapping\n",
    "state_normalization_map = {\n",
    "    # West Bengal variants\n",
    "    \"West Bengal\": \"West Bengal\",\n",
    "    \"West  Bengal\": \"West Bengal\",\n",
    "    \"West Bangal\": \"West Bengal\",\n",
    "    \"Westbengal\": \"West Bengal\",\n",
    "\n",
    "    # Odisha / Orissa\n",
    "    \"Orissa\": \"Odisha\",\n",
    "\n",
    "    # Jammu & Kashmir\n",
    "    \"Jammu & Kashmir\": \"Jammu And Kashmir\",\n",
    "\n",
    "    # Andaman & Nicobar\n",
    "    \"Andaman & Nicobar Islands\": \"Andaman And Nicobar Islands\",\n",
    "\n",
    "    # UT merger\n",
    "    \"Dadra & Nagar Haveli\": \"Dadra And Nagar Haveli And Daman And Diu\",\n",
    "    \"Daman & Diu\": \"Dadra And Nagar Haveli And Daman And Diu\",\n",
    "    \"Daman And Diu\": \"Dadra And Nagar Haveli And Daman And Diu\",\n",
    "    \"Dadra And Nagar Haveli\": \"Dadra And Nagar Haveli And Daman And Diu\",\n",
    "    \"The Dadra And Nagar Haveli And Daman And Diu\":\n",
    "        \"Dadra And Nagar Haveli And Daman And Diu\",\n",
    "\n",
    "    # Puducherry\n",
    "    \"Pondicherry\": \"Puducherry\"\n",
    "}\n",
    "\n",
    "biometric_clean[\"state\"] = (\n",
    "    biometric_clean[\"state\"]\n",
    "    .astype(str)\n",
    "    .str.strip()\n",
    "    .str.title()\n",
    "    .replace(state_normalization_map)\n",
    ")\n",
    "\n",
    "# Remove clearly invalid state values\n",
    "invalid_states = [\"100000\"]\n",
    "\n",
    "biometric_clean = biometric_clean[\n",
    "    ~biometric_combined[\"state\"].isin(invalid_states)\n",
    "]\n",
    "\n",
    "biometric_clean[\"state\"].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "be3c910b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Haryana', 'Bihar', 'Jammu And Kashmir', 'Tamil Nadu',\n",
       "       'Maharashtra', 'Gujarat', 'Odisha', 'West Bengal', 'Kerala',\n",
       "       'Rajasthan', 'Punjab', 'Himachal Pradesh', 'Uttar Pradesh',\n",
       "       'Assam', 'Uttarakhand', 'Madhya Pradesh', 'Karnataka',\n",
       "       'Andhra Pradesh', 'Telangana', 'Goa', 'Nagaland', 'Jharkhand',\n",
       "       'Delhi', 'Chhattisgarh', 'Meghalaya', 'Chandigarh', 'Puducherry',\n",
       "       'Manipur', 'Sikkim', 'Tripura', 'Mizoram', 'Arunachal Pradesh',\n",
       "       'Ladakh', 'Dadra And Nagar Haveli And Daman And Diu',\n",
       "       'Andaman And Nicobar Islands', 'Lakshadweep', 'Uttaranchal',\n",
       "       'Chhatisgarh', 'Tamilnadu'], dtype=object)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "biometric_clean[\"state\"].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a509c603",
   "metadata": {},
   "source": [
    "### Step 7D: Structural Standardization of District Names\n",
    "\n",
    "District names are standardized structurally to remove formatting noise while preserving\n",
    "original administrative semantics. No semantic remapping is performed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "43ab837a",
   "metadata": {},
   "outputs": [],
   "source": [
    "biometric_clean[\"district\"] = (\n",
    "    biometric_clean[\"district\"]\n",
    "    .astype(str)\n",
    "    .str.strip()\n",
    "    .str.replace(r\"\\s+\", \" \", regex=True)\n",
    "    .str.replace(r\"\\*\", \"\", regex=True)\n",
    "    .str.replace(r\"[()]\", \"\", regex=True)\n",
    "    .str.replace(\"–\", \"-\", regex=False)\n",
    "    .str.replace(\"−\", \"-\", regex=False)\n",
    "    .str.replace(\"?\", \"\", regex=False)\n",
    "    .str.title()\n",
    ")\n",
    "\n",
    "# Remove clearly invalid direction-only tokens\n",
    "invalid_districts = {\"East\", \"West\", \"North\", \"South\", \"North East\"}\n",
    "biometric_clean = biometric_clean[\n",
    "    ~biometric_clean[\"district\"].isin(invalid_districts)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "4b070d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop temporary provenance column\n",
    "if \"source_file\" in biometric_clean.columns:\n",
    "    biometric_clean = biometric_clean.drop(columns=[\"source_file\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "e6a42602",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 1859634 entries, 0 to 1861107\n",
      "Data columns (total 6 columns):\n",
      " #   Column        Dtype         \n",
      "---  ------        -----         \n",
      " 0   date          datetime64[ns]\n",
      " 1   state         object        \n",
      " 2   district      object        \n",
      " 3   pincode       int64         \n",
      " 4   bio_age_5_17  int64         \n",
      " 5   bio_age_17_   int64         \n",
      "dtypes: datetime64[ns](1), int64(3), object(2)\n",
      "memory usage: 99.3+ MB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>state</th>\n",
       "      <th>district</th>\n",
       "      <th>pincode</th>\n",
       "      <th>bio_age_5_17</th>\n",
       "      <th>bio_age_17_</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2025-03-01</td>\n",
       "      <td>Haryana</td>\n",
       "      <td>Mahendragarh</td>\n",
       "      <td>123029</td>\n",
       "      <td>280</td>\n",
       "      <td>577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2025-03-01</td>\n",
       "      <td>Bihar</td>\n",
       "      <td>Madhepura</td>\n",
       "      <td>852121</td>\n",
       "      <td>144</td>\n",
       "      <td>369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2025-03-01</td>\n",
       "      <td>Jammu And Kashmir</td>\n",
       "      <td>Punch</td>\n",
       "      <td>185101</td>\n",
       "      <td>643</td>\n",
       "      <td>1091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2025-03-01</td>\n",
       "      <td>Bihar</td>\n",
       "      <td>Bhojpur</td>\n",
       "      <td>802158</td>\n",
       "      <td>256</td>\n",
       "      <td>980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2025-03-01</td>\n",
       "      <td>Tamil Nadu</td>\n",
       "      <td>Madurai</td>\n",
       "      <td>625514</td>\n",
       "      <td>271</td>\n",
       "      <td>815</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        date              state      district  pincode  bio_age_5_17  bio_age_17_\n",
       "0 2025-03-01            Haryana  Mahendragarh   123029           280          577\n",
       "1 2025-03-01              Bihar     Madhepura   852121           144          369\n",
       "2 2025-03-01  Jammu And Kashmir         Punch   185101           643         1091\n",
       "3 2025-03-01              Bihar       Bhojpur   802158           256          980\n",
       "4 2025-03-01         Tamil Nadu       Madurai   625514           271          815"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "biometric_clean.info()\n",
    "biometric_clean.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6df211df",
   "metadata": {},
   "source": [
    "### Step 7 Summary\n",
    "\n",
    "The biometric update dataset now satisfies the following:\n",
    "- Dates are consistently parsed and temporally reliable\n",
    "- Biometric age-count fields are validated for numerical integrity\n",
    "- State names are canonical and aligned with other datasets\n",
    "- District names are structurally standardized without semantic loss\n",
    "- Temporary ingestion artifacts are removed\n",
    "\n",
    "The dataset is ready to be persisted for downstream analysis.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcecdb33",
   "metadata": {},
   "source": [
    "## Step 8: Persisting the Clean Biometric Update Dataset\n",
    "\n",
    "After completing all validation and cleaning steps, the biometric update dataset is finalized.\n",
    "In this step, the cleaned dataset is persisted to disk to serve as the single source of truth\n",
    "for all downstream analytical tasks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "cba039bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WindowsPath('../03_Processed_Data/biometric_update_clean.csv')"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define output directory\n",
    "OUTPUT_DIR = Path(\"../03_Processed_Data\")\n",
    "OUTPUT_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# Save cleaned biometric update dataset\n",
    "output_path = OUTPUT_DIR / \"biometric_update_clean.csv\"\n",
    "biometric_clean.to_csv(output_path, index=False)\n",
    "\n",
    "output_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "d8034060",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>state</th>\n",
       "      <th>district</th>\n",
       "      <th>pincode</th>\n",
       "      <th>bio_age_5_17</th>\n",
       "      <th>bio_age_17_</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2025-03-01</td>\n",
       "      <td>Haryana</td>\n",
       "      <td>Mahendragarh</td>\n",
       "      <td>123029</td>\n",
       "      <td>280</td>\n",
       "      <td>577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2025-03-01</td>\n",
       "      <td>Bihar</td>\n",
       "      <td>Madhepura</td>\n",
       "      <td>852121</td>\n",
       "      <td>144</td>\n",
       "      <td>369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2025-03-01</td>\n",
       "      <td>Jammu And Kashmir</td>\n",
       "      <td>Punch</td>\n",
       "      <td>185101</td>\n",
       "      <td>643</td>\n",
       "      <td>1091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2025-03-01</td>\n",
       "      <td>Bihar</td>\n",
       "      <td>Bhojpur</td>\n",
       "      <td>802158</td>\n",
       "      <td>256</td>\n",
       "      <td>980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2025-03-01</td>\n",
       "      <td>Tamil Nadu</td>\n",
       "      <td>Madurai</td>\n",
       "      <td>625514</td>\n",
       "      <td>271</td>\n",
       "      <td>815</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         date              state      district  pincode  bio_age_5_17  bio_age_17_\n",
       "0  2025-03-01            Haryana  Mahendragarh   123029           280          577\n",
       "1  2025-03-01              Bihar     Madhepura   852121           144          369\n",
       "2  2025-03-01  Jammu And Kashmir         Punch   185101           643         1091\n",
       "3  2025-03-01              Bihar       Bhojpur   802158           256          980\n",
       "4  2025-03-01         Tamil Nadu       Madurai   625514           271          815"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Quick verification\n",
    "pd.read_csv(output_path, nrows=5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
